---
layout: about
title: about
permalink: /
subtitle: <b>Robotics</b>, <b>3D Vision</b>

profile:
  align: right
  image: QianxuWang.jpeg
  image_circular: false # crops the image to make it circular
  # more_info: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder. -->

I am a fourth-year undergraduate student at Peking University, advised by <a href="https://yzhu.io/">Prof. Yixin Zhu</a> and fortunated collaborate with [Prof. Yaodong Yang](https://www.yangyaodong.com/) at PKU and <a href="https://geometry.stanford.edu/member/guibas/">Prof. Leonidas J. Guibas</a> at Stanford remotely. In 2024 summer, I worked as a visiting research student at Stanford, advised by <a href="https://web.stanford.edu/~bohg/">Prof. Jeannette Bohg</a>.


My long-term research goal is to achieve human-level robust sensorimotor coordination in robotics. I am also very interested in 3D Vision and Animation. My previous research has primarily focused on dexterous manipulation from a semantic perspective.

Currently, I am thinking and exploring <span style="color: #781eb0;">two key questions</span> in manipulation:

<strong style="color: #781eb0;">What are the sources of knowledge for manipulation?</strong> 

- **Shared information across datasets.** The features of *Cross-embodiment*, *cross-environment*, and *cross-quality* make robotic datasets unique compared to data in other fields like vision and natural language. Defining a universal data format and unifying existing datasets, rather than solely collecting new ones, presents a promising approach to fundamentally addressing data scarcity in robotics. I am eager to explore the structure of shared motion primitives and semantics in these datasets and investigate how to integrate them to achieve semantic-aware and robust manipulation in the real world.

- **Shared foundations with scalable data sources.** The vision domain offers rich semantic correspondences valuable for robotic perception, while natural language, as a natural carrier of reasoning and prompting, can enhance decision-making. I am excited to investigate the connections between robotics and scalable data sources by leveraging these shared foundations.

<strong style="color: #781eb0;">How can diverse sources of knowledge be effectively integrated?</strong>

- **Structured Policy** **Design**. Current policies (e.g. in IL/RL) directly map perception to actions of specific end-effector, which(i) process complex information without prioritization and (ii) limit the available data sources. In contrast, humans first reason about interactions visually, then adapt during manipulation using closed-loop feedback from multiple modalities (e.g., tactile, acoustic). I am excited to explore the design of a structured manipulation policy, including *how to integrate the end-effector agnostic action representation from diverse data* *sources* and *when to incorporate multi-modal perception and close-loop control*.


<strong style="color: #FF5733;">Feel free (and please do!) to reach out if you have any questions, comments about my research, or anything you'd like to discuss or share with me!</strong>

<!-- - Where does the info. come from?: I am curious about (1) Connecting robotics to more scalable data source given the shared foundations between robotics and vision, Language semantics; (2) formulating and distilling the shared information in cross-embodiment, cross-environment and cross-quality robotic dataset. 
- How to integrate diverse info.?: Compared to directly transfering perception into action in IL/RL's policies, humans reason about interactions through vision before making contact and then performing detailed adaptations with multi-modal (e.g. tactile and acoustic) perception. I am interested to explore when and how to integrate diverse information in robotics. -->

<!-- ### Misc

My dream is to be an artist in robotics.  -->


<!-- Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically. -->

<!-- Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

<!-- <a href="https://cs.stanford.edu/~congyue/">Congyue Deng</a> -->