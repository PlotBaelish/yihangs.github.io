---
layout: about
title: about
permalink: /
subtitle: <b>Robotics</b>, <b>3D Vision</b>

profile:
  align: right
  image: QianxuWang.jpeg
  image_circular: false # crops the image to make it circular
  # more_info: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder. -->

I am a fourth-year undergraduate student at Peking University, advised by <a href="https://yzhu.io/">Prof. Yixin Zhu</a> and fortunated collaborate with [Prof. Yaodong Yang](https://www.yangyaodong.com/) at PKU and <a href="https://geometry.stanford.edu/member/guibas/">Prof. Leonidas J. Guibas</a> at Stanford remotely. In 2024 summer, I worked as a visiting research student at Stanford, advised by <a href="https://web.stanford.edu/~bohg/">Prof. Jeannette Bohg</a>.

My long-term research goal is to achieve human-level robust sensorimotor coordination in robotics. My previous research mainly explore **dexterous manipulation** from the lens of **semantic**. 
Currently, I am thinking about two questions in manipulation: 


**Where does the information come from?**
- Understanding and distilling **shared information** in *cross-embodiment*, *cross-environment*, and *cross-quality* embodied dataset.
- Bridging robotics with **scalable data sources** by leveraging shared foundations between robotics, computer vision, and language
 
**How to integrate diverse information?**

- While current approaches directly map perception to action through IL/RL policies, humans first reason about interactions through vision, then adapt using multi-modal (e.g. tactile, acoustic) feedback. I am interested to explore when and how to integrate diverse information sources in robotics.


<!-- - Where does the info. come from?: I am curious about (1) Connecting robotics to more scalable data source given the shared foundations between robotics and vision, Language semantics; (2) formulating and distilling the shared information in cross-embodiment, cross-environment and cross-quality robotic dataset. 
- How to integrate diverse info.?: Compared to directly transfering perception into action in IL/RL's policies, humans reason about interactions through vision before making contact and then performing detailed adaptations with multi-modal (e.g. tactile and acoustic) perception. I am interested to explore when and how to integrate diverse information in robotics. -->

<!-- ### Misc

My dream is to be an artist in robotics.  -->


<!-- Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically. -->

<!-- Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

<!-- <a href="https://cs.stanford.edu/~congyue/">Congyue Deng</a> -->